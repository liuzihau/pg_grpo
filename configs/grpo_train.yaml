# Stage 3: GRPO on top of your KD LoRA
data:
  source: hf
  hf_name: allenai/tulu-3-sft-mixture
  messages_field: messages
  keep_history: true
  split: grpo
  max_input_len: 4096
  load_kwargs: {}
  sample_max: null

grpo:
  base_model_dir: outputs/kd/qwen3_draft_lora      # path containing kd cfg + adapters (or adapters in ./lora)
  out_dir: outputs/grpo/qwen3_grpo
  use_pregen_prompts: true
  pregen_dir: data/kd_corpus/qwen8b_S64_topk20
  pregen_split: grpo              # or validation
  offset_strategy: stride        # "uniform" | "stride"
  offset_stride: 8               # 0, 8, 16, ... up to cont_len-1
  cushion: 8                  # reserved tokens (guard room)
  batch_size: 2
  group_size: 8               # GRPO: samples per prompt
  total_steps: 2000
  lr: 5.0e-5
  weight_decay: 0.0
  betas: [0.9, 0.95]
  warmup_ratio: 0.05
  min_lr: 0.0
  max_new: 16 #128
  temperature: 1.5  # 0.8
  acceptance_cap: 1.0
  grad_accum_steps: 1
  log_every: 5
  teacher_device: cpu         # "auto" (default GPU) or "cpu" to fully offload teacher
  teacher_8bit: false         # set true if you have bitsandbytes and want GPU 8-bit teacher
  teacher_microbatch: 8
  teacher_device: "auto"        # or "cpu"
  teacher_8bit: false
  # regularization
  kl_ref: teacher             # teacher | draft_init | none
  kl_coeff: 0.01

  print_layers: true

training:
  seed: 1234
  device: cuda
  dtype: bf16
  grad_ckpt: true
  max_grad_norm: 1.0

logging:
  project: grpo-qwen
  name: qwen3_grpo_from_kd
